{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fyzeen/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import yaml\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import timm #only needed if downloading pretrained models\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models.sit import SiT\n",
    "from utils2.renm_utils import * #load_weights_imagenet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resolution': {'ico': 6, 'sub_ico': 2},\n",
       " 'data': {'data_path': '../data/{}/{}',\n",
       "  'task': 'ICAd15_schfd100',\n",
       "  'configuration': 'template',\n",
       "  'dataset': 'HCPdb',\n",
       "  'hemisphere': '1L'},\n",
       " 'logging': {'folder_to_save_model': '../logs/SiT/'},\n",
       " 'computation_opt': {'scale_grad_choice': False},\n",
       " 'training': {'LR': 1e-05,\n",
       "  'bs': 1,\n",
       "  'bs_val': 1,\n",
       "  'epochs': 10,\n",
       "  'gpu': 0,\n",
       "  'l1loss': False,\n",
       "  'testing': True,\n",
       "  'val_epoch': 2,\n",
       "  'load_weights_ssl': False,\n",
       "  'load_weights_imagenet': True,\n",
       "  'save_ckpt': True,\n",
       "  'finetuning': False,\n",
       "  'dataset_ssl': 'hcpdb',\n",
       "  'epoch_check': True},\n",
       " 'weights': {'ssl_mpp': '/scratch/naranjorincon/surface-vision-transformers/logs/SiT/0509-01:57-small-1L-ICAd15_schfd100-imgnet/checkpoint.pth',\n",
       "  'imagenet': 'vit_small_patch16_224'},\n",
       " 'transformer': {'dim': 384,\n",
       "  'depth': 12,\n",
       "  'heads': 6,\n",
       "  'mlp_dim': 1536,\n",
       "  'pool': 'mean',\n",
       "  'num_features': 15,\n",
       "  'num_classes': 4950,\n",
       "  'num_channels': 15,\n",
       "  'dim_head': 64,\n",
       "  'dropout': 0.3,\n",
       "  'emb_dropout': 0.1,\n",
       "  'model': 'SiT'},\n",
       " 'optimisation': {'optimiser': 'Adam'},\n",
       " 'Adam': {'weight_decay': 0.01},\n",
       " 'AdamW': {'weight_decay': 0.0},\n",
       " 'SGD': {'weight_decay': 0.0, 'momentum': 0.9, 'nesterov': False},\n",
       " 'StepLR': {'stepsize': 1000, 'decay': 0.5},\n",
       " 'CosineDecay': {'T_max': 5000, 'eta_min': 0.0001},\n",
       " 'sub_ico_0': {'num_patches': 20, 'num_vertices': 2145},\n",
       " 'sub_ico_1': {'num_patches': 80, 'num_vertices': 561},\n",
       " 'sub_ico_2': {'num_patches': 320, 'num_vertices': 153}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/config/SiT/training/ICAd15_schfd100.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/data/surf2mat/template/train_data.npy\")\n",
    "train_label = np.load(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/data/surf2mat/template/train_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_token_torch(tensor, start_value=1):\n",
    "    \"\"\"\n",
    "    Add a new column with a start value to the beginning of each sequence in the input tensor.\n",
    "    \n",
    "    :param tensor: Tensor of shape (batch_size, seq_length), input tensor\n",
    "    :param start_value: int, value to add at the start of each sequence\n",
    "    :return: Tensor of shape (batch_size, seq_length + 1), tensor with a new column added to the start of each sequence\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = tensor.size()\n",
    "    new_column = torch.full((batch_size, 1), start_value, dtype=tensor.dtype, device=tensor.device)  # Create a new column with the start value\n",
    "    out = torch.cat([new_column, tensor], dim=1)  # Concatenate the new column with the input tensor\n",
    "    return out\n",
    "\n",
    "def add_start_token_np(array, start_value=1):\n",
    "    \"\"\"\n",
    "    Add a new column with a start value to the beginning of each sequence in the input array.\n",
    "    \n",
    "    :param array: Array of shape (batch_size, seq_length), input array\n",
    "    :param start_value: int, value to add at the start of each sequence\n",
    "    :return: Array of shape (batch_size, seq_length + 1), array with a new column added to the start of each sequence\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = array.shape\n",
    "    new_column = np.full((batch_size, 1), start_value, dtype=array.dtype)  # Create a new column with the start value\n",
    "    out = np.concatenate((new_column, array), axis=1)  # Concatenate the new column with the input array\n",
    "    return out\n",
    "\n",
    "train_label = add_start_token_np(train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(), torch.from_numpy(train_label).float())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODIFIED SiT model from Dahan\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from vit_pytorch.vit import Transformer\n",
    "\n",
    "class EncoderSiT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        num_patches = 320,\n",
    "                        num_channels = 4,\n",
    "                        num_vertices = 153,\n",
    "                        dim_head = 64,\n",
    "                        sequence_length = 1225,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        patch_dim = num_channels * num_vertices\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dim = dim\n",
    "\n",
    "        # inputs has size = b * c * n * v where b = batch, c = channels, f = features, n=patches, v=verteces\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) # See here: https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "        self.linear = nn.Linear(num_patches * dim, sequence_length * dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        x += self.pos_embedding[:, :] # was originally sliced by [:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Reshape the input tensor to (batch_size, num_patches * dim)\n",
    "        x_reshaped = x.view(b, -1)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(x_reshaped)\n",
    "        # Reshape the output tensor to (batch_size, sequence_length, dim)\n",
    "        output = output.view(b, self.sequence_length, self.dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n",
      "torch.Size([2, 1225, 20])\n"
     ]
    }
   ],
   "source": [
    "model = EncoderSiT(dim=20,\n",
    "                   depth=5,\n",
    "                   heads=2, \n",
    "                   mlp_dim=40)\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    inputs, targets = data[0].to(device), data[1].to(device).squeeze()\n",
    "    model.to(device)\n",
    "    output = model(inputs)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.flatten_to_high_dim = nn.Linear(input_dim, input_dim * d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, seq_len=input_dim, dropout=dropout)\n",
    "        \n",
    "        self.masked_multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.cross_multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        b, _ = tgt.size()\n",
    "\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim(tgt)\n",
    "        tgt = tgt.view(b, -1, self.d_model)\n",
    "        print(tgt.shape)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "\n",
    "        tgt_mask = generate_subsequent_mask(self.input_dim).to(device)\n",
    "        # Masked Multi-Head Attention\n",
    "        tgt2, _ = self.masked_multihead_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)  # Residual connection\n",
    "        tgt = self.norm1(tgt)\n",
    "        print(tgt.shape)\n",
    "        \n",
    "        # Cross-Multi-Head Attention\n",
    "        tgt2, _ = self.cross_multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)  # Residual connection\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feed Forward\n",
    "        tgt2 = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout3(tgt2)  # Residual connection\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "def generate_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Generate a mask to ensure that each position in the sequence can only attend to\n",
    "    positions up to and including itself. This is a lower triangular matrix filled with ones.\n",
    "    \n",
    "    :param size: int, the length of the sequence\n",
    "    :return: tensor of shape (size, size), where element (i, j) is False if j <= i, and True otherwise (See attn_mask option here: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(size, size)).bool()\n",
    "    mask.diagonal().fill_(False)\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n",
      "torch.Size([2, 1226, 20])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderSiT(dim=20,\n",
    "                     depth=5,\n",
    "                     heads=2, \n",
    "                     mlp_dim=40)\n",
    "\n",
    "decoder = TransformerDecoderBlock(input_dim=1226,\n",
    "                                  d_model=20,\n",
    "                                  nhead=2,\n",
    "                                  dim_feedforward=80)\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    inputs, targets = data[0].to(device), data[1].to(device).squeeze()\n",
    "\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    encoder_out = encoder(inputs)\n",
    "    output = decoder(tgt = targets.to(device), memory=encoder_out)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
