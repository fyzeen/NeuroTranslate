{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from krakencoder_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xycorr(x,y,axis=1):\n",
    "    \"\"\"\n",
    "    **FROM KRAKENCODER.loss.py**\n",
    "\n",
    "    Compute correlation between all pairs of rows in x and y (or columns if axis=0)\n",
    "    \n",
    "    x: torch tensor or numpy array (Nsubj x M), generally the measured data for N subjects\n",
    "    y: torch tensor or numpy array (Nsubj x M), generally the predicted data for N subjects\n",
    "    axis: int (optional, default=1), 1 for row-wise, 0 for column-wise\n",
    "    \n",
    "    Returns: torch tensor or numpy array (Nsubj x Nsubj)\n",
    "    \n",
    "    NOTE: in train.py we always call cc=xycorr(Ctrue, Cpredicted)\n",
    "    which means cc[i,:] is cc[true subject i, predicted for all subjects]\n",
    "    and thus top1acc, which uses argmax(xycorr(true,predicted),axis=1) is:\n",
    "    for every TRUE output, which subject's PREDICTED output is the best match\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        cx=x-x.mean(keepdims=True,axis=axis)\n",
    "        cy=y-y.mean(keepdims=True,axis=axis)\n",
    "        cx=cx/torch.sqrt(torch.sum(cx ** 2,keepdims=True,axis=axis))\n",
    "        cy=cy/torch.sqrt(torch.sum(cy ** 2,keepdims=True,axis=axis))\n",
    "        cc=torch.matmul(cx,cy.t())\n",
    "    else:\n",
    "        cx=x-x.mean(keepdims=True,axis=axis)\n",
    "        cy=y-y.mean(keepdims=True,axis=axis)\n",
    "        cx=cx/np.sqrt(np.sum(cx ** 2,keepdims=True,axis=axis))\n",
    "        cy=cy/np.sqrt(np.sum(cy ** 2,keepdims=True,axis=axis))\n",
    "        cc=np.matmul(cx,cy.T)\n",
    "    return cc\n",
    "\n",
    "\n",
    "def correye(x,y):\n",
    "    \"\"\"\n",
    "    **FROM KRAKENCODER.loss.py**\n",
    "\n",
    "    Loss function: mean squared error between pairwise correlation matrix for xycorr(x,y) and identity matrix\n",
    "    (i.e., want diagonal to be near 1, off-diagonal to be near 0)\n",
    "    \"\"\"\n",
    "    cc=xycorr(x,y)\n",
    "    #need keepdim for some reason now that correye and enceye are separated\n",
    "    loss=torch.norm(cc-torch.eye(cc.shape[0],device=cc.device),keepdim=True)\n",
    "    return loss\n",
    "\n",
    "def distance_loss(x,y, margin=None, neighbor=False):\n",
    "    \"\"\"\n",
    "    **FROM KRAKENCODER.loss.py**\n",
    "\n",
    "    Loss function: difference between self-distance and other-distance for x and y, with optional margin\n",
    "    If neighbor=True, reconstruction loss applies only to nearest neighbor distance, otherwise to mean distance between all\n",
    "        off-diagonal pairs.\n",
    "    \n",
    "    Inputs:\n",
    "    x: torch tensor (Nsubj x M), generally the measured data\n",
    "    y: torch tensor (Nsubj x M), generally the predicted data\n",
    "    margin: float, optional margin for distance loss (distance above margin is penalized, below is ignored)\n",
    "    neighbor: bool, (optional, default=False), True for maximizing nearest neighbor distance, False for maximizing mean distance\n",
    "    \n",
    "    Returns: \n",
    "    loss: torch FloatTensor, difference between self-distance and other-distance\n",
    "    \"\"\"\n",
    "    \n",
    "    d=torch.cdist(x,y)\n",
    "    dtrace=torch.trace(d)\n",
    "    dself=dtrace/d.shape[0] #mean predicted->true distance -- avg distance x_subja to y_subja\n",
    "    \n",
    "    if neighbor:\n",
    "        dnei=d+torch.eye(d.shape[0],device=d.device)*d.max()\n",
    "        #mean of row-wise min and column-wise min\n",
    "        dother=torch.mean((dnei.min(axis=0)[0]+dnei.min(axis=1)[0])/2)\n",
    "    else:\n",
    "        dother=(torch.sum(d)-dtrace)/(d.shape[0]*(d.shape[0]-1)) #mean predicted->other distance\n",
    "    \n",
    "    if margin is not None:\n",
    "        #dother=torch.min(dother,margin)\n",
    "        #dother=-torch.nn.ReLU()(dother-margin) #pre 4/5/2024\n",
    "        #if dother<margin, penalize (lower = more penalty).\n",
    "        #if dother>=margin, ignore\n",
    "        #standard triplet loss: torch.nn.ReLU()(dself-dother+margin) or torch.clamp(dself-dother+margin,min=0)\n",
    "        dother=-torch.nn.ReLU()(margin-dother) #new 4/5/2024\n",
    "    \n",
    "    loss=dself-dother\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Target = np.load(f\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/SurfToNetmat/model_output/ICAd15_schfd100/ConvTransformer/train_ground_truth.npy\")\n",
    "test_Target = np.load(f\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/SurfToNetmat/model_output/ICAd15_schfd100/ConvTransformer/test_ground_truth.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=256)\n",
    "pca.fit(train_Target)\n",
    "\n",
    "train_transform = pca.transform(train_Target)\n",
    "test_transform = pca.transform(test_Target)\n",
    "\n",
    "train_transform = train_transform[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = np.random.randn(10, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_input).float(), torch.from_numpy(train_transform).float())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 2, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Krakencoder([256])\n",
    "\n",
    "def train(model, train_loader, device, optimizer, epoch, reset_params=True):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    targets_ = []\n",
    "    preds_ = []\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device).squeeze()#.unsqueeze(0) # USE THIS unsqueeze(0) ONLY if batch size = 1\n",
    "        \n",
    "        latent, pred = model(inputs, 0, 0)\n",
    "        \n",
    "        # Output Losses\n",
    "        Lr_corrI = correye(targets, pred) # corr mat of measured->predicted should be high along diagonal, loww off diagonal \n",
    "        Lr_mse = torch.nn.MSELoss()(pred, targets) # MSE should be low\n",
    "        Lr_marg = distance_loss(targets, pred, neighbor=True) # predicted X should be far from nearet ground truth X (for a different subject)\n",
    "\n",
    "        # Latent Space Losses\n",
    "        Lz_corrI = correye(latent, latent) # correlation matrix of latent space should be low off diagonal\n",
    "        Lz_dist = distance_loss(latent, latent, neighbor=False) / (targets.size()[0]**2) # mean intersubject altent space distances should be high\n",
    "\n",
    "        Lr = Lr_corrI + Lr_marg + (1000 * Lr_mse) \n",
    "        Lz = Lz_corrI + Lz_dist\n",
    "\n",
    "        loss = Lr + (10 * Lz) # weighting Lz with 10 (from Krakencoder)\n",
    "\n",
    "\n",
    "        mae = torch.nn.L1Loss()(pred, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        targets_.append(targets.cpu().numpy())\n",
    "        preds_.append(pred.cpu().detach().numpy())\n",
    "\n",
    "    return targets_, preds_, loss, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-119.6116]], grad_fn=<AddBackward0>)\n",
      "tensor(0.0091, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m601\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     targets_, preds_, loss, mae \u001b[38;5;241m=\u001b[39m train(model, train_loader, device, optimizer, epoch)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mae)\n",
      "Cell \u001b[0;32mIn[65], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, device, optimizer, epoch, reset_params)\u001b[0m\n\u001b[1;32m      7\u001b[0m targets_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m preds_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     11\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;66;03m#.unsqueeze(0) # USE THIS unsqueeze(0) ONLY if batch size = 1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     latent, pred \u001b[38;5;241m=\u001b[39m model(inputs, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1317\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_workers()\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1442\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     w\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1444\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize model on device\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "# initialize optimizer / loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, eps=1e-9)\n",
    "\n",
    "for epoch in range(0, 601):\n",
    "    targets_, preds_, loss, mae = train(model, train_loader, device, optimizer, epoch)\n",
    "    print(loss)\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotranslate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
