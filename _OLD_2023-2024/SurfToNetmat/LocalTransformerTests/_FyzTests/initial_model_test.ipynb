{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fyzeen/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import yaml\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timm #only needed if downloading pretrained models\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from utils2.renm_utils import * #load_weights_imagenet\n",
    "import random\n",
    "\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from vit_pytorch.vit import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/data/surf2mat/template/train_data.npy\")\n",
    "train_label = np.load(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/data/surf2mat/template/train_labels.npy\")\n",
    "\n",
    "\n",
    "\n",
    "def add_start_token_torch(tensor, start_value=1):\n",
    "    \"\"\"\n",
    "    Add a new column with a start value to the beginning of each sequence in the input tensor.\n",
    "    \n",
    "    :param tensor: Tensor of shape (batch_size, seq_length), input tensor\n",
    "    :param start_value: int, value to add at the start of each sequence\n",
    "    :return: Tensor of shape (batch_size, seq_length + 1), tensor with a new column added to the start of each sequence\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = tensor.size()\n",
    "    new_column = torch.full((batch_size, 1), start_value, dtype=tensor.dtype, device=tensor.device)  # Create a new column with the start value\n",
    "    out = torch.cat([new_column, tensor], dim=1)  # Concatenate the new column with the input tensor\n",
    "    return out\n",
    "\n",
    "def add_start_token_np(array, start_value=1):\n",
    "    \"\"\"\n",
    "    Add a new column with a start value to the beginning of each sequence in the input array.\n",
    "    \n",
    "    :param array: Array of shape (batch_size, seq_length), input array\n",
    "    :param start_value: int, value to add at the start of each sequence\n",
    "    :return: Array of shape (batch_size, seq_length + 1), array with a new column added to the start of each sequence\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = array.shape\n",
    "    new_column = np.full((batch_size, 1), start_value, dtype=array.dtype)  # Create a new column with the start value\n",
    "    out = np.concatenate((new_column, array), axis=1)  # Concatenate the new column with the input array\n",
    "    return out\n",
    "\n",
    "# taking only first two subjects and first 100 cells in the parcellation\n",
    "train_label = train_label[:1, :100]\n",
    "train_data = train_data[:1, :, :, :]\n",
    "\n",
    "train_label = add_start_token_np(train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(), torch.from_numpy(train_label).float())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Generate a mask to ensure that each position in the sequence can only attend to\n",
    "    positions up to and including itself. This is a lower triangular matrix filled with ones.\n",
    "    \n",
    "    :param size: int, the length of the sequence\n",
    "    :return: tensor of shape (size, size), where element (i, j) is False if j <= i, and True otherwise (See attn_mask option here: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(size, size)).bool()\n",
    "    mask.diagonal().fill_(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, input_dim, device, b=2): #b=batch size\n",
    "    '''\n",
    "    Greedy decode algorithm for a full encoder-decoder architecture (inference).\n",
    "\n",
    "    Implements using ALL options (initialization with torch.ones vs zeros vs randn; generating ONLY i+1th val VS :i+1 tokens on each iteration)\n",
    "    '''\n",
    "    encoder_output = model.encode(source)\n",
    "    initialization_list = [torch.zeros(b,input_dim).to(device), torch.ones(b,input_dim).to(device), torch.randn(b,input_dim).to(device)]\n",
    "    \n",
    "    # build target mask\n",
    "    decoder_mask = generate_subsequent_mask(initialization_list[0].size(1)).to(device)\n",
    "\n",
    "    out_list = []\n",
    "\n",
    "    for i, decoder_input in enumerate(initialization_list):\n",
    "        decoder_input_copy = decoder_input.clone()\n",
    "        for i in range(input_dim-1):\n",
    "            # compute next output\n",
    "            out = model.decode(encoder_out=encoder_output, tgt=decoder_input, tgt_mask=decoder_mask)\n",
    "            decoder_input[:, i+1] = out.squeeze(1)[:, i+1]\n",
    "\n",
    "        out_list.append(decoder_input.squeeze(0))\n",
    "\n",
    "        for i in range(input_dim-1):\n",
    "            out = model.decode(encoder_out=encoder_output, tgt=decoder_input_copy, tgt_mask=decoder_mask)\n",
    "            decoder_input_copy[:, :i+1] = out.squeeze(1)[:, :i+1] \n",
    "        \n",
    "        out_list.append(decoder_input_copy.squeeze(0))\n",
    "\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mses(mses):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Mean Squared Errors (MSEs) Over Iterations')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.plot(mses)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullTransformer(dim_model=20, \n",
    "                        encoder_depth=5,\n",
    "                        nhead = 2,\n",
    "                        encoder_mlp_dim=80,\n",
    "                        decoder_input_dim=101,\n",
    "                        decoder_dim_feedforward=80,\n",
    "                        decoder_depth=5,\n",
    "                        dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 0: 0.18584363162517548\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 0: 0.19281522929668427\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 1: 0.19106265902519226\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 1: 0.1921955645084381\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 2: 0.23258045315742493\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 2: 0.20905910432338715\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 3: 0.22530755400657654\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 3: 0.20205941796302795\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 4: 0.2379966378211975\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 4: 0.1964716911315918\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 5: 0.23316511511802673\n",
      "torch.Size([1, 4, 320, 153])\n",
      "torch.Size([1, 101])\n",
      "Train Loss, Epoch 5: 0.2062973976135254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neurotranslate/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# like a train function\n",
    "model.to(device)\n",
    "#model._reset_parameters() reset if you wanna restart\n",
    "\n",
    "trainlosses = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, eps=1e-9)\n",
    "global_step = 0\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device).squeeze().unsqueeze(0) # USE THIS unsqueeze(0) ONLY if batch size = 1\n",
    "\n",
    "        print(inputs.shape)\n",
    "        print(targets.shape)\n",
    "        \n",
    "        pred = model(src=inputs, tgt=targets, tgt_mask=generate_subsequent_mask(101).to(device))\n",
    "\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        print(f\"Train Loss, Epoch {epoch}: {loss}\")\n",
    "        trainlosses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        global_step+=1\n",
    "    \n",
    "    if (epoch % 25 == 0) and epoch>0:\n",
    "        #plot_mses(trainlosses)\n",
    "        break\n",
    "\n",
    "    #torch.save(model.state_dict(), \"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/_FyzTests/torchsave.pt\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullTransformer(\n",
       "  (flatten_to_high_dim): Linear(in_features=101, out_features=2020, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): EncoderSiT(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c n v  -> b n (v c)')\n",
       "      (1): Linear(in_features=612, out_features=20, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-4): 5 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (to_qkv): Linear(in_features=20, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=20, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=20, out_features=80, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "              (4): Linear(in_features=80, out_features=20, bias=True)\n",
       "              (5): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=6400, out_features=24500, bias=True)\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-4): 5 x TransformerDecoderBlock(\n",
       "      (masked_multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "      )\n",
       "      (cross_multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=20, out_features=80, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=80, out_features=20, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=2020, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/fyzeen/FyzeenLocal/GitHub/NeuroTranslate/LocalTransformerTests/_FyzTests/torchsave.pt\"))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fyzeen/opt/miniconda3/envs/neurotranslate/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343668887/work/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    }
   ],
   "source": [
    "targets_ = []\n",
    "preds_ = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device).squeeze().unsqueeze(0)\n",
    "        print(targets.shape)\n",
    "\n",
    "        predictions = greedy_decode(model=model, source=inputs, input_dim=101, device=device, b=1)\n",
    "\n",
    "        targets_.append(targets.cpu().numpy())\n",
    "        preds_.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ 0.0000, -0.0012, -0.2333,  0.0283, -0.0626, -0.0278,  0.0190,  0.0650,\n",
       "           0.0293, -0.1221, -0.0468, -0.0365,  0.0700, -0.1215, -0.0069, -0.0789,\n",
       "           0.0489, -0.0974,  0.0722, -0.0711,  0.0138, -0.0056,  0.0720,  0.1411,\n",
       "          -0.0706, -0.0844, -0.0241, -0.0993, -0.0778, -0.1968, -0.0922, -0.0545,\n",
       "           0.0340, -0.0179, -0.0769,  0.0472,  0.0780,  0.1329,  0.0369,  0.0494,\n",
       "           0.1386, -0.0062,  0.1717, -0.0839,  0.0495, -0.0064, -0.0580,  0.0181,\n",
       "          -0.1644, -0.1207,  0.0155, -0.1651, -0.0338, -0.0375, -0.0615,  0.0658,\n",
       "          -0.0311, -0.0154, -0.0289, -0.0670, -0.0782, -0.0160,  0.1308, -0.0201,\n",
       "           0.0133, -0.0255, -0.0775,  0.0808,  0.0275,  0.1124, -0.0071, -0.0862,\n",
       "          -0.0218, -0.0736, -0.0714,  0.0319,  0.0111,  0.1312,  0.1367, -0.0236,\n",
       "          -0.0871,  0.0510,  0.1205, -0.0584, -0.1066,  0.0437,  0.0313,  0.0409,\n",
       "           0.0020,  0.0817, -0.1449,  0.1471, -0.0926,  0.1592,  0.0373,  0.0681,\n",
       "          -0.0157, -0.0181,  0.1086, -0.1228, -0.1050]),\n",
       "  tensor([ 9.9301e-01,  1.0489e-03, -2.3346e-01,  3.1382e-02, -6.6352e-02,\n",
       "          -2.6996e-02,  1.7070e-02,  6.1001e-02,  2.7571e-02, -1.2208e-01,\n",
       "          -4.5281e-02, -3.8320e-02,  6.9215e-02, -1.2198e-01, -4.4534e-03,\n",
       "          -7.4919e-02,  4.8341e-02, -9.8898e-02,  7.2243e-02, -7.0356e-02,\n",
       "           1.4079e-02, -2.5844e-03,  7.3304e-02,  1.4191e-01, -7.2634e-02,\n",
       "          -7.9827e-02, -2.4968e-02, -9.8154e-02, -7.7921e-02, -1.9562e-01,\n",
       "          -9.1131e-02, -5.2575e-02,  3.2671e-02, -2.0844e-02, -7.5709e-02,\n",
       "           4.5648e-02,  7.7425e-02,  1.3527e-01,  3.6336e-02,  5.2118e-02,\n",
       "           1.3494e-01, -2.7856e-03,  1.7162e-01, -8.3208e-02,  5.0673e-02,\n",
       "          -7.6001e-03, -5.6371e-02,  1.8285e-02, -1.6543e-01, -1.2136e-01,\n",
       "           1.6431e-02, -1.6588e-01, -3.4548e-02, -3.4777e-02, -5.9293e-02,\n",
       "           6.6558e-02, -3.0738e-02, -1.4317e-02, -2.7489e-02, -6.9439e-02,\n",
       "          -8.2522e-02, -1.3617e-02,  1.2863e-01, -2.3034e-02,  1.5039e-02,\n",
       "          -2.7396e-02, -7.8435e-02,  8.3625e-02,  2.5762e-02,  1.1251e-01,\n",
       "          -4.4988e-03, -8.8086e-02, -2.2656e-02, -7.1647e-02, -7.2680e-02,\n",
       "           3.3888e-02,  9.7292e-03,  1.3422e-01,  1.3953e-01, -2.2986e-02,\n",
       "          -8.7169e-02,  5.0842e-02,  1.2005e-01, -5.8495e-02, -1.0645e-01,\n",
       "           4.4616e-02,  3.2432e-02,  3.9983e-02,  9.3288e-04,  8.1306e-02,\n",
       "          -1.4529e-01,  1.4281e-01, -9.3094e-02,  1.5712e-01,  3.6399e-02,\n",
       "           7.0882e-02, -1.3674e-02, -1.8262e-02,  1.1083e-01, -1.2211e-01,\n",
       "           0.0000e+00]),\n",
       "  tensor([ 1.0000, -0.0223, -0.2389,  0.0141, -0.0393, -0.0344,  0.0190,  0.0775,\n",
       "           0.0501, -0.1242, -0.0421, -0.0266,  0.0554, -0.1312, -0.0144, -0.0906,\n",
       "           0.0723, -0.0901,  0.0898, -0.0659,  0.0295, -0.0157,  0.0815,  0.1342,\n",
       "          -0.0911, -0.0876, -0.0076, -0.0955, -0.0830, -0.2006, -0.0773, -0.0463,\n",
       "           0.0305, -0.0162, -0.0582,  0.0405,  0.0846,  0.1386,  0.0219,  0.0574,\n",
       "           0.1286,  0.0094,  0.1771, -0.0671,  0.0399, -0.0076, -0.0472,  0.0091,\n",
       "          -0.1661, -0.1096,  0.0185, -0.1755, -0.0420, -0.0201, -0.0824,  0.0729,\n",
       "          -0.0295, -0.0264, -0.0400, -0.0694, -0.0760, -0.0285,  0.1234, -0.0226,\n",
       "           0.0101, -0.0209, -0.0852,  0.0808,  0.0304,  0.1038, -0.0141, -0.0710,\n",
       "          -0.0162, -0.0713, -0.0732,  0.0298,  0.0070,  0.1353,  0.1411, -0.0095,\n",
       "          -0.0928,  0.0565,  0.1257, -0.0561, -0.1113,  0.0521,  0.0308,  0.0442,\n",
       "           0.0025,  0.0876, -0.1368,  0.1410, -0.0936,  0.1534,  0.0358,  0.0723,\n",
       "          -0.0150, -0.0225,  0.1135, -0.1189, -0.1032]),\n",
       "  tensor([ 9.9299e-01, -8.2201e-04, -2.3445e-01,  2.7803e-02, -6.5859e-02,\n",
       "          -2.9984e-02,  1.5533e-02,  5.9361e-02,  2.8945e-02, -1.2138e-01,\n",
       "          -4.6060e-02, -3.6616e-02,  6.9709e-02, -1.2385e-01, -5.1783e-03,\n",
       "          -7.4890e-02,  5.0071e-02, -9.9242e-02,  7.3441e-02, -6.9467e-02,\n",
       "           1.1177e-02, -3.8013e-03,  7.0777e-02,  1.4218e-01, -7.4344e-02,\n",
       "          -8.0018e-02, -2.2463e-02, -9.1549e-02, -8.0727e-02, -1.9618e-01,\n",
       "          -8.8297e-02, -5.6250e-02,  3.1968e-02, -2.0234e-02, -7.2454e-02,\n",
       "           4.4263e-02,  7.6325e-02,  1.3309e-01,  3.7154e-02,  5.3393e-02,\n",
       "           1.3368e-01, -2.2062e-03,  1.6865e-01, -7.9195e-02,  5.0293e-02,\n",
       "          -7.9373e-03, -5.5138e-02,  1.6949e-02, -1.6419e-01, -1.2133e-01,\n",
       "           1.6590e-02, -1.6855e-01, -3.2584e-02, -3.2083e-02, -5.8334e-02,\n",
       "           6.9937e-02, -3.0152e-02, -1.2523e-02, -3.1695e-02, -6.8363e-02,\n",
       "          -8.3095e-02, -1.2106e-02,  1.2785e-01, -2.1854e-02,  1.8944e-02,\n",
       "          -2.6729e-02, -7.6246e-02,  8.2460e-02,  2.7284e-02,  1.1245e-01,\n",
       "          -8.9521e-03, -8.5676e-02, -2.1508e-02, -7.1990e-02, -7.2558e-02,\n",
       "           3.2720e-02,  6.5856e-03,  1.3234e-01,  1.4215e-01, -2.7262e-02,\n",
       "          -8.5279e-02,  5.2810e-02,  1.1836e-01, -5.8692e-02, -1.0595e-01,\n",
       "           4.6492e-02,  3.3429e-02,  4.1097e-02,  4.6630e-04,  8.1607e-02,\n",
       "          -1.4229e-01,  1.4261e-01, -9.0616e-02,  1.5472e-01,  3.5232e-02,\n",
       "           6.8826e-02, -1.2015e-02, -2.1763e-02,  1.1443e-01, -1.1899e-01,\n",
       "           1.0000e+00]),\n",
       "  tensor([ 0.3887, -0.0061, -0.2335,  0.0112, -0.0715, -0.0291,  0.0218,  0.0541,\n",
       "           0.0302, -0.1062, -0.0615, -0.0161,  0.0845, -0.1289,  0.0123, -0.0569,\n",
       "           0.0746, -0.1105,  0.0687, -0.0525,  0.0203,  0.0066,  0.0503,  0.1477,\n",
       "          -0.0765, -0.0731, -0.0179, -0.0969, -0.0660, -0.1979, -0.0803, -0.0729,\n",
       "           0.0309, -0.0243, -0.0633,  0.0524,  0.0888,  0.1372,  0.0360,  0.0438,\n",
       "           0.1351, -0.0142,  0.1865, -0.0778,  0.0426, -0.0184, -0.0667,  0.0240,\n",
       "          -0.1540, -0.1185,  0.0137, -0.1654, -0.0298, -0.0333, -0.0638,  0.0585,\n",
       "          -0.0350, -0.0120, -0.0276, -0.0694, -0.0729, -0.0270,  0.1307, -0.0201,\n",
       "           0.0213, -0.0235, -0.0774,  0.0856,  0.0376,  0.1261, -0.0084, -0.0915,\n",
       "          -0.0242, -0.0744, -0.0690,  0.0343,  0.0186,  0.1279,  0.1471, -0.0288,\n",
       "          -0.0885,  0.0597,  0.1186, -0.0632, -0.1126,  0.0470,  0.0323,  0.0442,\n",
       "           0.0040,  0.0785, -0.1424,  0.1476, -0.0931,  0.1557,  0.0332,  0.0696,\n",
       "          -0.0131, -0.0192,  0.1089, -0.1212, -0.1054]),\n",
       "  tensor([ 0.9930,  0.0021, -0.2344,  0.0309, -0.0668, -0.0281,  0.0166,  0.0587,\n",
       "           0.0281, -0.1226, -0.0443, -0.0376,  0.0688, -0.1229, -0.0052, -0.0753,\n",
       "           0.0504, -0.0996,  0.0735, -0.0698,  0.0155, -0.0033,  0.0739,  0.1420,\n",
       "          -0.0726, -0.0812, -0.0266, -0.0981, -0.0802, -0.1960, -0.0914, -0.0530,\n",
       "           0.0324, -0.0213, -0.0752,  0.0461,  0.0754,  0.1362,  0.0346,  0.0520,\n",
       "           0.1345, -0.0027,  0.1710, -0.0816,  0.0497, -0.0073, -0.0547,  0.0199,\n",
       "          -0.1648, -0.1205,  0.0150, -0.1660, -0.0341, -0.0348, -0.0595,  0.0682,\n",
       "          -0.0310, -0.0141, -0.0283, -0.0671, -0.0830, -0.0141,  0.1303, -0.0217,\n",
       "           0.0155, -0.0251, -0.0773,  0.0829,  0.0269,  0.1128, -0.0048, -0.0875,\n",
       "          -0.0226, -0.0722, -0.0735,  0.0330,  0.0104,  0.1335,  0.1398, -0.0256,\n",
       "          -0.0880,  0.0524,  0.1201, -0.0598, -0.1099,  0.0448,  0.0319,  0.0400,\n",
       "           0.0015,  0.0808, -0.1443,  0.1423, -0.0921,  0.1562,  0.0348,  0.0697,\n",
       "          -0.0129, -0.0192,  0.1105, -0.1210, -0.3074])]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mae_epoch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mconcatenate(targets_[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(preds_[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())))\n\u001b[1;32m      2\u001b[0m mae_epoch\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "mae_epoch = np.mean(np.abs(np.concatenate(targets_[0]) - np.concatenate(preds_[0][1].cpu().numpy())))\n",
    "mae_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1975, device='mps:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "loss_fn(pred, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  5.47885336e-02  1.11828014e-01 -7.64793307e-02\n",
      "  -3.72464880e-02  6.47229403e-02  8.78072307e-02 -1.02245107e-01\n",
      "  -3.53789702e-02  1.31891251e-01 -4.49974351e-02  1.91501789e-02\n",
      "  -6.17876165e-02  2.79100388e-02 -2.18449114e-03 -2.35218555e-02\n",
      "  -1.32850170e-01  1.09109387e-01  3.00634652e-03  2.81155631e-02\n",
      "  -3.93006466e-02  1.02485843e-01 -2.96451151e-02 -3.85278836e-02\n",
      "  -1.37200058e-02 -4.43901941e-02 -4.68282700e-02  5.52757978e-02\n",
      "   2.55711637e-02  6.91241026e-02  8.09423774e-02 -1.08139217e-03\n",
      "   2.63378620e-02  4.06384747e-03  2.13998556e-02 -5.50149493e-02\n",
      "  -1.24481253e-01 -8.41911137e-02  6.76657632e-03 -8.53681192e-03\n",
      "  -4.32314202e-02  7.63071477e-02 -9.01077166e-02  7.78756812e-02\n",
      "  -2.94941068e-02  2.01291982e-02 -4.88891527e-02  6.59624115e-03\n",
      "   1.12655088e-01  2.78413221e-02 -2.35273652e-02  1.23063624e-02\n",
      "   8.73373002e-02  5.58957160e-02  7.24873170e-02 -9.17304009e-02\n",
      "  -6.25871494e-03  8.31624866e-02  4.20214459e-02  9.85423028e-02\n",
      "   2.25876868e-02  7.38012716e-02  1.11558884e-01  4.40953448e-02\n",
      "   5.35446499e-03  1.21051021e-01  3.59790884e-02 -7.91032612e-03\n",
      "  -5.57356924e-02 -7.58736879e-02 -8.30534697e-02  3.57771553e-02\n",
      "  -2.20369119e-02  1.00173905e-01 -4.83222306e-02 -3.71876545e-02\n",
      "   1.45436479e-02 -1.71430081e-01 -1.08514667e-01  6.05327040e-02\n",
      "   1.04346409e-01 -7.64261484e-02 -2.52508298e-02  7.32046887e-02\n",
      "   5.26360460e-02  3.19635198e-02  1.93074122e-02 -9.19782296e-02\n",
      "  -1.41078257e-04 -5.72093949e-02  9.17518437e-02 -9.18160379e-02\n",
      "   1.69651285e-02 -1.05884686e-01 -4.99025136e-02 -1.39949381e-01\n",
      "  -5.42360544e-02 -1.16939671e-01 -7.82141760e-02  1.25767291e-03\n",
      "   6.91232979e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(targets_[0] - preds_[0][0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         0.053635  -0.12151   -0.04817   -0.099836   0.036915\n",
      "   0.10685   -0.037254  -0.0060648  0.009801  -0.091813  -0.017395\n",
      "   0.0082223 -0.093588  -0.0091054 -0.10244   -0.083986   0.011728\n",
      "   0.075165  -0.042984  -0.025545   0.096871   0.042341   0.10258\n",
      "  -0.084292  -0.12884   -0.070937  -0.043987  -0.052232  -0.12772\n",
      "  -0.011292  -0.055536   0.06038   -0.013792  -0.055451  -0.0078369\n",
      "  -0.046508   0.048743   0.043623   0.04084    0.095379   0.070152\n",
      "   0.081631  -0.0060417  0.019975   0.013762  -0.10691    0.024739\n",
      "  -0.051731  -0.092834  -0.0079792 -0.15281    0.053569   0.018373\n",
      "   0.011028  -0.02589   -0.037309   0.067792   0.013131   0.031553\n",
      "  -0.055607   0.057753   0.2424     0.023996   0.018646   0.095537\n",
      "  -0.041546   0.072851  -0.028209   0.036517  -0.090177  -0.050468\n",
      "  -0.043845   0.026591  -0.11971   -0.0052818  0.025643  -0.04022\n",
      "   0.028181   0.036962   0.01726   -0.025386   0.095228   0.014827\n",
      "  -0.054002   0.0757     0.050593  -0.051038   0.0019048  0.024536\n",
      "  -0.053113   0.055319  -0.075651   0.053315  -0.012569  -0.07182\n",
      "  -0.069892  -0.13501    0.030409  -0.12152   -0.03586  ]]\n"
     ]
    }
   ],
   "source": [
    "print(targets_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000, -0.0012, -0.2333,  0.0283, -0.0626, -0.0278,  0.0190,  0.0650,\n",
      "         0.0293, -0.1221, -0.0468, -0.0365,  0.0700, -0.1215, -0.0069, -0.0789,\n",
      "         0.0489, -0.0974,  0.0722, -0.0711,  0.0138, -0.0056,  0.0720,  0.1411,\n",
      "        -0.0706, -0.0844, -0.0241, -0.0993, -0.0778, -0.1968, -0.0922, -0.0545,\n",
      "         0.0340, -0.0179, -0.0769,  0.0472,  0.0780,  0.1329,  0.0369,  0.0494,\n",
      "         0.1386, -0.0062,  0.1717, -0.0839,  0.0495, -0.0064, -0.0580,  0.0181,\n",
      "        -0.1644, -0.1207,  0.0155, -0.1651, -0.0338, -0.0375, -0.0615,  0.0658,\n",
      "        -0.0311, -0.0154, -0.0289, -0.0670, -0.0782, -0.0160,  0.1308, -0.0201,\n",
      "         0.0133, -0.0255, -0.0775,  0.0808,  0.0275,  0.1124, -0.0071, -0.0862,\n",
      "        -0.0218, -0.0736, -0.0714,  0.0319,  0.0111,  0.1312,  0.1367, -0.0236,\n",
      "        -0.0871,  0.0510,  0.1205, -0.0584, -0.1066,  0.0437,  0.0313,  0.0409,\n",
      "         0.0020,  0.0817, -0.1449,  0.1471, -0.0926,  0.1592,  0.0373,  0.0681,\n",
      "        -0.0157, -0.0181,  0.1086, -0.1228, -0.1050])\n"
     ]
    }
   ],
   "source": [
    "print(preds_[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.44203997],\n",
       "       [0.44203997, 1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(pred[1].detach().cpu().numpy(), targets[1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionConv(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ProjectionConv, self).__init__()\n",
    "    self.projection_conv = nn.Conv1d(in_channels=100, out_channels=360, kernel_size=1, groups=10)\n",
    "\n",
    "  \n",
    "  def forward(self, input):\n",
    "    B, C = input.shape\n",
    "    output = self.projection_conv(input.unsqueeze(-1))\n",
    "    return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([360, 10, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ProjectionConv()\n",
    "\n",
    "\n",
    "\n",
    "#model(torch.ones(1, 100))\n",
    "\n",
    "model.projection_conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotranslate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
