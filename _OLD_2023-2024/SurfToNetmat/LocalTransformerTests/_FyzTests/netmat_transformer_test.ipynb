{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vit_pytorch.vit import Transformer\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class EncoderSiT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        dim_head,\n",
    "                        output_length = 512,\n",
    "                        num_channels = 15,\n",
    "                        num_patches = 320,\n",
    "                        num_vertices = 153,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "        patch_dim = num_channels * num_vertices\n",
    "\n",
    "        self.output_length = output_length\n",
    "        self.dim = dim\n",
    "\n",
    "        # inputs has size = b * c * n * v where b = batch, c = channels, f = features, n=patches, v=verteces\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) # See here: https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "        self.linear = nn.Linear(num_patches * dim, output_length * dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        x += self.pos_embedding[:, :] # was originally sliced by [:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Reshape the input tensor to (batch_size, num_patches * dim)\n",
    "        x_reshaped = x.view(b, -1)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(x_reshaped)\n",
    "        # Reshape the output tensor to (batch_size, output_length, dim)\n",
    "        output = output.view(b, self.output_length, self.dim)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.masked_multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.cross_multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Masked Multi-Head Attention\n",
    "        tgt2, masked_attn_weights = self.masked_multihead_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)  # Residual connection\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-Multi-Head Attention\n",
    "        tgt2, cross_attn_weights = self.cross_multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)  # Residual connection\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feed Forward\n",
    "        tgt2 = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout3(tgt2)  # Residual connection\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt, masked_attn_weights, cross_attn_weights\n",
    "\n",
    "class ProjectionConvFullTransformer(nn.Module):\n",
    "    def __init__(self, dim_model, encoder_depth, nhead, encoder_mlp_dim, decoder_input_dim, decoder_dim_feedforward, decoder_depth, dim_encoder_head, \n",
    "                 latent_length=512, num_channels=15, dropout=0.1, num_patches=320, vertices_per_patch=153):\n",
    "        super(ProjectionConvFullTransformer, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.input_dim = decoder_input_dim\n",
    "        self.latent_length = latent_length\n",
    "\n",
    "        self.flatten_to_high_dim = nn.Conv1d(in_channels=decoder_input_dim, out_channels=latent_length*dim_model, kernel_size=1, groups=latent_length)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=dim_model, seq_len=latent_length, dropout=dropout)\n",
    "\n",
    "        self.encoder = EncoderSiT(dim=dim_model, \n",
    "                                  depth=encoder_depth, \n",
    "                                  heads=nhead, \n",
    "                                  mlp_dim=encoder_mlp_dim,\n",
    "                                  dim_head=dim_encoder_head,\n",
    "                                  num_channels=num_channels,  \n",
    "                                  num_patches=num_patches, \n",
    "                                  num_vertices=vertices_per_patch, \n",
    "                                  dropout=dropout,\n",
    "                                  output_length=latent_length,\n",
    "                                  emb_dropout=0.1)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(input_dim=decoder_input_dim, d_model=dim_model, nhead=nhead, dim_feedforward=decoder_dim_feedforward) for _ in range(decoder_depth)])\n",
    "\n",
    "        self.projection = nn.Conv1d(in_channels=latent_length*dim_model, out_channels=decoder_input_dim, kernel_size=1, groups=latent_length)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask):\n",
    "        b, _ = tgt.size()\n",
    "\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "                \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "\n",
    "        return tgt #torch.tanh(tgt) \n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, dropout=0.1):\n",
    "        b, _ = tgt.size()\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        latent = encoder_out = self.encoder(src)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "        \n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "        \n",
    "        return tgt.squeeze(), latent #torch.tanh(tgt.squeeze()), latent\n",
    "    \n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, dim_model, encoder_depth, nhead, encoder_mlp_dim, decoder_input_dim, decoder_dim_feedforward, decoder_depth, dim_encoder_head, \n",
    "                 latent_length=512, num_channels=15, dropout=0.1, num_patches=320, vertices_per_patch=153):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.input_dim = decoder_input_dim\n",
    "        self.latent_length = latent_length\n",
    "\n",
    "        self.encoder = EncoderSiT(dim=dim_model, \n",
    "                                  depth=encoder_depth, \n",
    "                                  heads=nhead, \n",
    "                                  mlp_dim=encoder_mlp_dim,\n",
    "                                  dim_head=dim_encoder_head,\n",
    "                                  num_channels=num_channels,  \n",
    "                                  num_patches=num_patches, \n",
    "                                  num_vertices=vertices_per_patch, \n",
    "                                  dropout=dropout,\n",
    "                                  output_length=latent_length,\n",
    "                                  emb_dropout=0.1)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(input_dim=decoder_input_dim, d_model=dim_model, nhead=nhead, dim_feedforward=decoder_dim_feedforward) for _ in range(decoder_depth)])\n",
    "\n",
    "        self.projection = nn.Conv1d(in_channels=latent_length*dim_model, out_channels=latent_length*dim_model, kernel_size=1, groups=latent_length)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask):\n",
    "        b, _, _ = tgt.size()\n",
    "\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.transpose(1, 2).reshape(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model).transpose(2,1)\n",
    "\n",
    "        return torch.tanh(tgt.squeeze()) \n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, dropout=0.1):\n",
    "        b, _, _ = tgt.size()\n",
    "\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "        encoder_out = self.encoder(src)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.transpose(1, 2).reshape(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model).transpose(2,1)\n",
    "\n",
    "        return torch.tanh(tgt.squeeze())\n",
    "    \n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, *args, mask, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight*self.mask, self.bias)\n",
    "\n",
    "\n",
    "class TriuGraphTransformer(nn.Module):\n",
    "    def __init__(self, dim_model, encoder_depth, nhead, encoder_mlp_dim, decoder_input_dim, decoder_dim_feedforward, decoder_depth, dim_encoder_head, num_out_nodes=100,\n",
    "                 latent_length=512, num_channels=15, dropout=0.1, num_patches=320, vertices_per_patch=153, extra_start_tokens=1):\n",
    "        super(TriuGraphTransformer, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.input_dim = decoder_input_dim\n",
    "        self.latent_length = latent_length\n",
    "        self.extra_start_tokens = extra_start_tokens\n",
    "\n",
    "        self.encoder = EncoderSiT(dim=dim_model, \n",
    "                                  depth=encoder_depth, \n",
    "                                  heads=nhead, \n",
    "                                  mlp_dim=encoder_mlp_dim,\n",
    "                                  dim_head=dim_encoder_head,\n",
    "                                  num_channels=num_channels,  \n",
    "                                  num_patches=num_patches, \n",
    "                                  num_vertices=vertices_per_patch, \n",
    "                                  dropout=dropout,\n",
    "                                  output_length=latent_length,\n",
    "                                  emb_dropout=0.1)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(input_dim=decoder_input_dim, d_model=dim_model, nhead=nhead, dim_feedforward=decoder_dim_feedforward) for _ in range(decoder_depth)])\n",
    "\n",
    "        self.projection = MaskedLinear(in_features=latent_length*dim_model, out_features=int((num_out_nodes * (num_out_nodes-1)) / 2), mask=create_mask(num_out_nodes=num_out_nodes, latent_length=latent_length, num_extra_start_tokens=extra_start_tokens))\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask):\n",
    "        b, _, _ = tgt.size()\n",
    "\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.transpose(1, 2).reshape(b, -1)\n",
    "        tgt = self.projection(tgt)\n",
    "\n",
    "        return torch.tanh(tgt.squeeze()) \n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, dropout=0.1):\n",
    "        b, _, _ = tgt.size()\n",
    "\n",
    "        tgt = tgt.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "        encoder_out = self.encoder(src)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.transpose(1, 2).reshape(b, -1)\n",
    "        tgt = self.projection(tgt)\n",
    "\n",
    "        return torch.tanh(tgt.squeeze())\n",
    "\n",
    "# From Samuel below:\n",
    "class SiT_nopool_linout(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        num_patches = 320,\n",
    "                        num_classes= 4950,\n",
    "                        num_channels = 15,\n",
    "                        num_vertices = 153,\n",
    "                        dim_head = 64,\n",
    "                        dropout = 0.3,\n",
    "                        emb_dropout = 0.1\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # features of maps only add to number of patches, but dim of each patch stays at 4*153\n",
    "        patch_dim = num_channels * num_vertices # flattened patch\n",
    "\n",
    "        # linear embedding of the vectorized patches\n",
    "        # inputs has size = b * c * n * v, I think this changes depending on input featues for meshes, so \n",
    "        # size = b * c * f * n * v where b = batch c = channels f = features, n=patches?, v=verteces?\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        ) # linear layer embeds the inputdim*153 -> attention dim\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim)) # plus one for regressino token according to paper\n",
    "        # good guide I used to walkthrough: https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) # torch transformer\n",
    "\n",
    "        decomp_attnpatch = num_patches * dim # size of decomposed patch and their attention vector\n",
    "\n",
    "        self.rearrange = Rearrange('b n d  -> b (n d)') # decomp them here, which will be the size of decomp_attnpatch\n",
    "\n",
    "        self.linear = nn.Linear(decomp_attnpatch,num_classes) # linear project from batch x 122k -> batch 4950\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, img):\n",
    "        #write_to_file('Looking into vectorized brain maps shape: {}'.format(img.size())) # should have size batch, chan, sphere(s) count, patches, verteces  \n",
    "        x = self.to_patch_embedding(img)\n",
    "        _, n, _ = x.shape # look above at to_patch_embedding see that it 'b c n v  -> b n (v c)' 256x320x384 if training at least\n",
    "        #write_to_file('Performed Patch embedding, and has shape:{}'.format(x.shape))\n",
    "\n",
    "        x += self.pos_embedding[:,:(n)] # spatial relationship across patches based on pos embedding of tokens\n",
    "        #write_to_file('Performed pos emb, now has shape: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #write_to_file('Dropout used, now has shape: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.transformer(x) # give embedded input to transformer architecture\n",
    "        #write_to_file('Passed through transformer architecture, now has shape: {}'.format(x.shape))\n",
    "\n",
    "        x = latent = self.rearrange(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        #write_to_file('Collapsed patchesxattndim, now projected linearly to num_classes - has shape: {}'.format(x.shape))\n",
    "        \n",
    "        return x, latent\n",
    "    \n",
    "class VariationalSiT_nopool_linout(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        VAE_latent_dim = 500,\n",
    "                        num_patches = 320,\n",
    "                        num_classes= 4950,\n",
    "                        num_channels = 15,\n",
    "                        num_vertices = 153,\n",
    "                        dim_head = 64,\n",
    "                        dropout = 0.3,\n",
    "                        emb_dropout = 0.1\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # features of maps only add to number of patches, but dim of each patch stays at 4*153\n",
    "        patch_dim = num_channels * num_vertices # flattened patch\n",
    "\n",
    "        # linear embedding of the vectorized patches\n",
    "        # inputs has size = b * c * n * v, I think this changes depending on input featues for meshes, so \n",
    "        # size = b * c * f * n * v where b = batch c = channels f = features, n=patches?, v=verteces?\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        ) # linear layer embeds the inputdim*153 -> attention dim\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim)) \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) # torch transformer\n",
    "\n",
    "        decomp_attnpatch = num_patches * dim # size of decomposed patch and their attention vector\n",
    "\n",
    "        self.rearrange = Rearrange('b n d  -> b (n d)') # decomp them here, which will be the size of decomp_attnpatch\n",
    "\n",
    "        self.fc_mu = nn.Linear(decomp_attnpatch, VAE_latent_dim) # linear project from batch x 122k -> batch 500\n",
    "        self.fc_var = nn.Linear(decomp_attnpatch, VAE_latent_dim)\n",
    "        \n",
    "        self.projection = nn.Linear(VAE_latent_dim, num_classes)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, img):\n",
    "        #write_to_file('Looking into vectorized brain maps shape: {}'.format(img.size())) # should have size batch, chan, sphere(s) count, patches, verteces  \n",
    "        x = self.to_patch_embedding(img)\n",
    "        _, n, _ = x.shape # look above at to_patch_embedding see that it 'b c n v  -> b n (v c)' 256x320x384 if training at least\n",
    "        #write_to_file('Performed Patch embedding, and has shape:{}'.format(x.shape))\n",
    "\n",
    "        x += self.pos_embedding[:,:(n)] # spatial relationship across patches based on pos embedding of tokens\n",
    "        #write_to_file('Performed pos emb, now has shape: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #write_to_file('Dropout used, now has shape: {}'.format(x.shape))\n",
    "        \n",
    "        x = self.transformer(x) # give embedded input to transformer architecture\n",
    "        #write_to_file('Passed through transformer architecture, now has shape: {}'.format(x.shape))\n",
    "\n",
    "        x = self.rearrange(x)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        x = mu + (std * epsilon)\n",
    "\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x, mu, log_var\n",
    "\n",
    "class VariationalConvTransformer(nn.Module):\n",
    "    def __init__(self, dim_model, encoder_depth, nhead, encoder_mlp_dim, decoder_input_dim, decoder_dim_feedforward, decoder_depth, dim_encoder_head, \n",
    "                 VAE_latent_dim=1000, latent_length=512, num_channels=15, dropout=0.1, num_patches=320, vertices_per_patch=153):\n",
    "        super(VariationalConvTransformer, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.input_dim = decoder_input_dim\n",
    "        self.latent_length = latent_length\n",
    "\n",
    "        self.flatten_to_high_dim = nn.Conv1d(in_channels=decoder_input_dim, out_channels=latent_length*dim_model, kernel_size=1, groups=latent_length)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=dim_model, seq_len=latent_length, dropout=dropout)\n",
    "\n",
    "        self.encoder = EncoderSiT(dim=dim_model, \n",
    "                                  depth=encoder_depth, \n",
    "                                  heads=nhead, \n",
    "                                  mlp_dim=encoder_mlp_dim,\n",
    "                                  dim_head=dim_encoder_head,\n",
    "                                  num_channels=num_channels,  \n",
    "                                  num_patches=num_patches, \n",
    "                                  num_vertices=vertices_per_patch, \n",
    "                                  dropout=dropout,\n",
    "                                  output_length=latent_length,\n",
    "                                  emb_dropout=0.1)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(dim_model * latent_length, VAE_latent_dim)\n",
    "        self.fc_var = nn.Linear(dim_model * latent_length, VAE_latent_dim)\n",
    "\n",
    "        self.vae_latent_to_encoder_out = nn.Linear(VAE_latent_dim, dim_model * latent_length)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(input_dim=decoder_input_dim, d_model=dim_model, nhead=nhead, dim_feedforward=decoder_dim_feedforward) for _ in range(decoder_depth)])\n",
    "\n",
    "        self.projection = nn.Conv1d(in_channels=latent_length*dim_model, out_channels=decoder_input_dim, kernel_size=1, groups=latent_length)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def encode(self, src):\n",
    "        x = self.encoder(src)\n",
    "        x = x.view(x.size()[0], -1) # reshape to [b x model_dim * latent_length]\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "\n",
    "        return [mu, log_var]\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask):\n",
    "        b, _ = tgt.size()\n",
    "\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "                \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # Reparameterization trick to sample from latent space\n",
    "        mu = encoder_out[0]\n",
    "        log_var = encoder_out[1]\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + (std * epsilon)\n",
    "\n",
    "        vae_in_encoder_space = self.vae_latent_to_encoder_out(z)\n",
    "        vae_in_encoder_space = vae_in_encoder_space.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=vae_in_encoder_space, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "\n",
    "        return tgt #torch.tanh(tgt) \n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, dropout=0.1):\n",
    "        b, _ = tgt.size()\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        encoder_out = self.encode(src)\n",
    "        \n",
    "        # Reparameterization trick to sample from latent space\n",
    "        mu = encoder_out[0]\n",
    "        log_var = encoder_out[1]\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + (std * epsilon)\n",
    "\n",
    "        vae_in_encoder_space = self.vae_latent_to_encoder_out(z)\n",
    "        vae_in_encoder_space = vae_in_encoder_space.view(b, self.latent_length, self.dim_model)\n",
    "\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=vae_in_encoder_space, tgt_mask=tgt_mask)\n",
    "        \n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "        \n",
    "        return tgt.squeeze(), mu, log_var #torch.tanh(tgt.squeeze()), mu, log_var\n",
    "    \n",
    "\n",
    "class TwoHemi_SiT_nopool_linout(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        latent_dim = 500,\n",
    "                        num_patches = 320,\n",
    "                        num_classes= 4950,\n",
    "                        num_channels = 15,\n",
    "                        num_vertices = 153,\n",
    "                        dim_head = 64,\n",
    "                        dropout = 0.3,\n",
    "                        emb_dropout = 0.1\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        patch_dim = num_channels * num_vertices # flattened patch\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.elu  = nn.ELU()\n",
    "\n",
    "        # HEMI 1\n",
    "        self.hemi1_to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        ) \n",
    "\n",
    "        self.hemi1_pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.hemi1_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) \n",
    "\n",
    "        decomp_attnpatch = num_patches * dim \n",
    "\n",
    "        self.rearrange = Rearrange('b n d  -> b (n d)') \n",
    "\n",
    "        self.hemi1_linear = nn.Linear(decomp_attnpatch, latent_dim) # linear project from batch x 122k -> batch x latent_dim\n",
    "\n",
    "        # HEMI 2\n",
    "        self.hemi2_to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        ) \n",
    "\n",
    "        self.hemi2_pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "\n",
    "        self.hemi2_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) \n",
    "\n",
    "        self.hemi2_linear = nn.Linear(decomp_attnpatch, latent_dim) # linear project from batch x 122k -> batch x latent_dim\n",
    "\n",
    "        # share hemi information and project --- !IMPORTANT! FOR \"LARGE\" (not \"LARGER\" models), output of sharehemis should be `int(num_classes/2)`\n",
    "        self.share_hemis = nn.Linear(latent_dim*2, int(num_classes))\n",
    "        self.project = nn.Linear(int(num_classes), int(num_classes))\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, hemi1, hemi2):\n",
    "\n",
    "        x = self.hemi1_to_patch_embedding(hemi1)\n",
    "        _, n, _ = x.shape \n",
    "        x += self.hemi1_pos_embedding[:,:(n)]       \n",
    "        x = self.dropout(x)        \n",
    "        x = self.hemi1_transformer(x)\n",
    "        x = self.rearrange(x)\n",
    "        x = self.hemi1_linear(x) \n",
    "        hemi1_latent = x#self.relu(x)\n",
    "\n",
    "        x = self.hemi2_to_patch_embedding(hemi2)\n",
    "        _, n, _ = x.shape \n",
    "        x += self.hemi2_pos_embedding[:,:(n)]       \n",
    "        x = self.dropout(x)        \n",
    "        x = self.hemi2_transformer(x)\n",
    "        x = self.rearrange(x)\n",
    "        x = self.hemi2_linear(x) \n",
    "        hemi2_latent = x#self.relu(x)\n",
    "        \n",
    "        x = torch.cat((hemi1_latent, hemi2_latent), dim=1)\n",
    "        x = latent = self.share_hemis(x)\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        x = self.project(x)\n",
    "\n",
    "        return x, latent\n",
    "    \n",
    "\n",
    "class NetmatEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 dim_model, \n",
    "                 nhead, \n",
    "                 num_layers, \n",
    "                 dim_feedforward,\n",
    "                 output_length, \n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(NetmatEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_length = output_length\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model=dim_model, seq_len=input_dim, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(input_dim * dim_model, output_length * dim_model)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        x = self.positional_encoding(src)\n",
    "        x = self.transformer_encoder(src)\n",
    "\n",
    "        b, seq_len, dim_model = x.size()\n",
    "\n",
    "        x = x.view(b, seq_len * dim_model)\n",
    "        print(x.size())\n",
    "        x = self.linear(x)\n",
    "        x = x.view(b, self.output_length, self.dim_model)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ProjectionConvTransformerNetmat(nn.Module):\n",
    "    def __init__(self, dim_model, encoder_input_dim, encoder_model_len, encoder_depth, nhead_encoder, encoder_mlp_dim, decoder_input_dim, nhead_decoder, decoder_dim_feedforward, decoder_depth, \n",
    "                 latent_length=512, dropout=0.1):\n",
    "        super(ProjectionConvTransformerNetmat, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.input_dim = decoder_input_dim\n",
    "        self.latent_length = latent_length\n",
    "\n",
    "        # encoder\n",
    "        self.flatten_to_high_dim_encoder = nn.Conv1d(in_channels=encoder_input_dim, out_channels=encoder_model_len*dim_model, kernel_size=1, groups=encoder_model_len)\n",
    "        self.positional_encoding_encoder = PositionalEncoding(d_model=dim_model, seq_len=encoder_model_len, dropout=dropout)\n",
    "\n",
    "        self.encoder = NetmatEncoder(input_dim = encoder_model_len,\n",
    "                                     dim_model = dim_model, \n",
    "                                     nhead = nhead_encoder, \n",
    "                                     num_layers = encoder_depth, \n",
    "                                     dim_feedforward = encoder_mlp_dim,\n",
    "                                     output_length = latent_length)\n",
    "        \n",
    "        # decoder\n",
    "        self.flatten_to_high_dim_decoder = nn.Conv1d(in_channels=decoder_input_dim, out_channels=latent_length*dim_model, kernel_size=1, groups=latent_length)\n",
    "        self.positional_encoding_decoder = PositionalEncoding(d_model=dim_model, seq_len=latent_length, dropout=dropout)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(input_dim=decoder_input_dim, d_model=dim_model, nhead=nhead_decoder, dim_feedforward=decoder_dim_feedforward) for _ in range(decoder_depth)])\n",
    "\n",
    "        self.projection = nn.Conv1d(in_channels=latent_length*dim_model, out_channels=decoder_input_dim, kernel_size=1, groups=latent_length)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def encode(self, src):\n",
    "        b, _ = src.size()\n",
    "\n",
    "        # Project to high-dimensional space\n",
    "        x = self.flatten_to_high_dim_encoder(src.unsqueeze(-1))\n",
    "        x = x.view(b, -1, self.dim_model)\n",
    "                \n",
    "        # Apply positional encoding\n",
    "        x = self.positional_encoding_encoder(x)\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask):\n",
    "        b, _ = tgt.size()\n",
    "\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim_decoder(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "                \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding_decoder(tgt)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "\n",
    "        return tgt #torch.tanh(tgt) \n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, dropout=0.1):\n",
    "        b, _ = tgt.size()\n",
    "        # Project to high-dimensional space\n",
    "        tgt = self.flatten_to_high_dim_decoder(tgt.unsqueeze(-1))\n",
    "        tgt = tgt.view(b, -1, self.dim_model)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        tgt = self.positional_encoding_decoder(tgt)\n",
    "\n",
    "        latent = encoder_out = self.encode(src)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt, masked_attn_weights, cross_attn_weights = layer(tgt=tgt, memory=encoder_out, tgt_mask=tgt_mask)\n",
    "        \n",
    "        tgt = tgt.view(b, -1)\n",
    "        tgt = self.projection(tgt.unsqueeze(-1))\n",
    "        \n",
    "        return tgt.squeeze(), latent #torch.tanh(tgt.squeeze()), latent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Generate a mask to ensure that each position in the sequence can only attend to\n",
    "    positions up to and including itself. This is a upper triangular matrix filled with ones.\n",
    "    \n",
    "    :param size: int, the length of the sequence\n",
    "    :return: tensor of shape (size, size), where element (i, j) is False if j <= i, and True otherwise (See attn_mask option here: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(size, size)).bool()\n",
    "    mask.diagonal().fill_(False)\n",
    "    return mask.bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProjectionConvTransformerNetmat(dim_model=100,\n",
    "                                        encoder_input_dim=360,\n",
    "                                        encoder_model_len=36, \n",
    "                                        nhead_encoder=10,\n",
    "                                        nhead_decoder=10, \n",
    "                                        decoder_depth=2,\n",
    "                                        encoder_depth=2,\n",
    "                                        encoder_mlp_dim=100,\n",
    "                                        decoder_input_dim=264,\n",
    "                                        decoder_dim_feedforward=100,\n",
    "                                        latent_length=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3600])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 264])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(32, 360)\n",
    "output = torch.randn(32, 264)\n",
    "\n",
    "model(input, output, tgt_mask=generate_subsequent_mask(model.latent_length))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotranslate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
